
























"""
SecOps æ™ºèƒ½ä½“ - åŸºäº LangChain é‡æ„ç‰ˆæœ¬
æ”¯æŒ Streaming ä¸­é—´æ­¥éª¤æ¨é€å’Œåˆ†é˜¶æ®µæ‰§è¡Œ
"""
import json
import logging
import asyncio
from typing import Dict, Any, List, Optional, AsyncGenerator
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from django.conf import settings

from app.agent_tools.hexstrike_tools import HexStrikeProgressiveTool
from app.services.task_tools import (
    create_task,
    list_tasks,
    update_task,
    parse_cron_from_natural_language,
    list_assets,
)
from app.utils.sse_manager import SSEManager

logger = logging.getLogger(__name__)


class SecOpsLangChainAgent:
    """
    åŸºäº LangChain çš„ SecOps æ™ºèƒ½ä½“

    ç‰¹ç‚¹ï¼š
    1. æ”¯æŒ Streaming ä¸­é—´æ­¥éª¤æ¨é€
    2. é›†æˆ LangChain å·¥å…·ç”Ÿæ€
    3. å¼‚æ­¥æ‰§è¡Œï¼Œä¸é˜»å¡
    4. å®æ—¶åé¦ˆç”¨æˆ·

    æ³¨æ„ï¼šé€‚é… langchain 1.2.x ç‰ˆæœ¬
    """

    def __init__(
        self,
        api_key: str,
        model: str = "qwen-plus",
        temperature: float = 0.3,
    ):
        """
        åˆå§‹åŒ– LangChain Agent

        Args:
            api_key: é€šä¹‰åƒé—® API Key
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
        """
        self.api_key = api_key
        self.model = model
        self.temperature = temperature

        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            api_key=api_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            model=model,
            temperature=temperature,
            streaming=True,  # å¯ç”¨æµå¼è¾“å‡º
        )

        # å®šä¹‰å·¥å…·
        self.tools = self._create_tools()

        logger.info(f"SecOps LangChain Agent åˆå§‹åŒ–å®Œæˆ: model={model}, tools={len(self.tools)}")

    def _create_tools(self) -> List[BaseTool]:
        """åˆ›å»ºå·¥å…·åˆ—è¡¨"""
        tools = [
            # HexStrike åˆ†é˜¶æ®µæ‰«æå·¥å…·
            HexStrikeProgressiveTool(),
        ]

        return tools

    def _find_tool(self, tool_name: str) -> Optional[BaseTool]:
        """æ ¹æ®åç§°æŸ¥æ‰¾å·¥å…·"""
        for tool in self.tools:
            if tool.name == tool_name:
                return tool
        return None

    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®‰å…¨è¿è¥(SecOps)æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æ‰§è¡Œå®‰å…¨è¿è¥ä»»åŠ¡ã€‚

ä½ çš„æ ¸å¿ƒèƒ½åŠ›ï¼š
1. å®‰å…¨è¯„ä¼°ï¼šä½¿ç”¨ hexstrike_progressive_scan å·¥å…·å¯¹ç›®æ ‡è¿›è¡Œåˆ†é˜¶æ®µæ‰«æ
2. ä»»åŠ¡ç®¡ç†ï¼šåˆ›å»ºã€æŸ¥è¯¢ã€æ›´æ–°å®šæ—¶ä»»åŠ¡
3. èµ„äº§æŸ¥è¯¢ï¼šæŸ¥è¯¢ç³»ç»Ÿä¸­çš„èµ„äº§åˆ—è¡¨
4. æ¼æ´é‡‡é›†ã€èµ„äº§é‡‡é›†ç­‰è¿è¥ä»»åŠ¡

é‡è¦æç¤ºï¼š
- å½“ç”¨æˆ·è¦æ±‚å¯¹æŸèµ„äº§åšå®‰å…¨è¯„ä¼°æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ hexstrike_progressive_scan å·¥å…·
- hexstrike_progressive_scan ä¼šåˆ†é˜¶æ®µæ‰§è¡Œï¼š
  * é˜¶æ®µ1: Ping ä¸»æœºå­˜æ´»æ£€æµ‹ï¼ˆç§’çº§ï¼‰
  * é˜¶æ®µ2: å¿«é€Ÿç«¯å£æ‰«æï¼ˆ10-30ç§’ï¼‰
  * é˜¶æ®µ3: åå°å®Œæ•´æ‰«æ + æ¼æ´æ£€æµ‹ï¼ˆåˆ†é’Ÿçº§ï¼‰
- åªå¯¹ç”¨æˆ·æ‹¥æœ‰æˆ–æ˜ç¡®æˆæƒçš„èµ„äº§è¿›è¡Œè¯„ä¼°

å·¥ä½œæµç¨‹ï¼š
1. ç†è§£ç”¨æˆ·æ„å›¾
2. å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œä½¿ç”¨ TOOLS æ ¼å¼ï¼š`{{"tool": "tool_name", "input": {{...}}}}`
3. å·¥å…·æ‰§è¡Œå®Œæˆåï¼Œæ€»ç»“ç»“æœ
4. æä¾›ä¸“ä¸šå»ºè®®

è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›å¤ç”¨æˆ·ã€‚"""

    async def astream_chat(
        self,
        message: str,
        user_id: Optional[str] = None,
        chat_history: Optional[List[Dict]] = None,
    ) -> AsyncGenerator[str, None]:
        """
        å¼‚æ­¥æµå¼å¯¹è¯ï¼ˆæ”¯æŒä¸­é—´æ­¥éª¤æ¨é€ï¼‰

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            user_id: ç”¨æˆ·IDï¼ˆç”¨äº SSE æ¨é€ï¼‰
            chat_history: å¯¹è¯å†å²

        Yields:
            str: å“åº”æ–‡æœ¬ç‰‡æ®µ
        """
        # åˆå§‹åŒ– SSE ç®¡ç†å™¨
        channel = f"user_{user_id}" if user_id else "chat_progress"
        sse = SSEManager(channel)

        try:
            # ç«‹å³å“åº”
            yield "ğŸ¤– æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...\n\n"

            # æ„å»ºæ¶ˆæ¯å†å²
            messages = [SystemMessage(content=self._build_system_prompt())]

            # æ·»åŠ å†å²æ¶ˆæ¯
            if chat_history:
                for msg in chat_history:
                    if msg["role"] == "user":
                        messages.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        messages.append(AIMessage(content=msg["content"]))

            # æ·»åŠ å½“å‰æ¶ˆæ¯
            messages.append(HumanMessage(content=message))

            # ç®€å•çš„å·¥å…·è°ƒç”¨é€»è¾‘ï¼ˆä¸ä½¿ç”¨ AgentExecutorï¼‰
            # _process_with_tools æ˜¯ async generatorï¼Œéœ€è¦è¿­ä»£
            async for chunk in self._process_with_tools(messages, sse, user_id):
                yield chunk

            # æ¨é€å®Œæˆäº‹ä»¶
            sse.send_complete({"final_output": "å“åº”å®Œæˆ"})

        except Exception as e:
            logger.error(f"Agent æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            error_msg = f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}"
            yield error_msg
            sse.send_error(error_msg)

    async def _process_with_tools(
        self,
        messages: List,
        sse: SSEManager,
        user_id: Optional[str] = None
    ) -> str:
        """
        å¤„ç†æ¶ˆæ¯ï¼ˆå¸¦å·¥å…·è°ƒç”¨ï¼‰

        è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œä¸ä¾èµ– AgentExecutor
        """
        # ç¬¬ä¸€æ­¥ï¼šè®© LLM å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        decision_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚

å¯ç”¨å·¥å…·ï¼š
{tool_descriptions}

å¦‚æœç”¨æˆ·çš„è¯·æ±‚éœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼å›å¤ï¼š
```json
{{
    "need_tool": true,
    "tool": "tool_name",
    "input": {{
        "parameter1": "value1",
        "parameter2": "value2"
    }}
}}
```

å¦‚æœä¸éœ€è¦è°ƒç”¨å·¥å…·ï¼Œç›´æ¥å›å¤ç”¨æˆ·å³å¯ã€‚"""),
            MessagesPlaceholder(variable_name="messages"),
        ])

        tool_descriptions = "\n".join([
            f"- {tool.name}: {tool.description}"
            for tool in self.tools
        ])

        decision_chain = decision_prompt | self.llm | StrOutputParser()

        # è·å–å†³ç­–
        decision_input = {
            "tool_descriptions": tool_descriptions,
            "messages": messages
        }

        decision_result = ""
        async for chunk in decision_chain.astream(decision_input):
            decision_result += chunk
            # å®æ—¶æµå¼è¾“å‡ºå†³ç­–è¿‡ç¨‹ï¼ˆè®©ç”¨æˆ·çœ‹åˆ° AI æ­£åœ¨æ€è€ƒï¼‰
            yield chunk

        logger.info(f"LLM å†³ç­–: {decision_result}")

        # å°è¯•è§£æå·¥å…·è°ƒç”¨
        try:
            # æå– JSON
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', decision_result, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªç»“æœ
                json_str = decision_result

            tool_call = json.loads(json_str)

            if tool_call.get("need_tool") and tool_call.get("tool"):
                # éœ€è¦è°ƒç”¨å·¥å…·
                tool_name = tool_call["tool"]
                tool_input = tool_call.get("input", {})

                tool = self._find_tool(tool_name)
                if tool:
                    # æ‰§è¡Œå·¥å…·
                    sse.send_tool_start(tool_name, tool_input)
                    logger.info(f"è°ƒç”¨å·¥å…·: {tool_name}, input: {tool_input}")

                    # Yieldå·¥å…·å¼€å§‹æç¤ºç»™å‰ç«¯
                    yield f"\nğŸ”§ æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_name}\n"
                    yield f"ğŸ“Š å‚æ•°: {json.dumps(tool_input, ensure_ascii=False)}\n\n"

                    # è°ƒç”¨å·¥å…·
                    if hasattr(tool, '_arun'):
                        tool_result = await tool._arun(**tool_input)
                    else:
                        tool_result = tool._run(**tool_input)

                    sse.send_tool_end(tool_name, str(tool_result)[:500])

                    # Yieldå·¥å…·æ‰§è¡Œç»“æœç»™å‰ç«¯
                    if tool_result.get("success"):
                        task_id = tool_result.get("task_id")
                        if task_id:
                            yield f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸï¼\n"
                            yield f"ğŸ“‹ åå°ä»»åŠ¡ID: `{task_id}`\n"

                        # æ˜¾ç¤ºå¿«é€Ÿæ‰«æç»“æœ
                        quick_scan = tool_result.get("quick_scan", {})
                        ports = quick_scan.get("ports", [])
                        if ports:
                            yield f"\nğŸ” å¿«é€Ÿæ‰«æç»“æœï¼š\n"
                            yield f"å‘ç° {len(ports)} ä¸ªå¼€æ”¾ç«¯å£ï¼š\n"
                            for port_info in ports[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                                yield f"  - ç«¯å£ {port_info.get('port')}/{port_info.get('protocol')}: {port_info.get('state')}\n"
                            if len(ports) > 10:
                                yield f"  ... è¿˜æœ‰ {len(ports) - 10} ä¸ªç«¯å£\n"
                        else:
                            yield f"âš ï¸  æœªå‘ç°å¼€æ”¾ç«¯å£ï¼ˆå¯èƒ½ç›®æ ‡ç¦ç”¨äº†ICMPæˆ–é˜²ç«å¢™é˜»æ­¢ï¼‰\n"
                    else:
                        error = tool_result.get("error", "æœªçŸ¥é”™è¯¯")
                        yield f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {error}\n"

                    # è®© LLM æ€»ç»“å·¥å…·ç»“æœ
                    summary_prompt = ChatPromptTemplate.from_messages([
                        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®‰å…¨è¿è¥åŠ©æ‰‹ã€‚æ ¹æ®å·¥å…·æ‰§è¡Œç»“æœï¼Œç»™ç”¨æˆ·ä¸€ä¸ªæ¸…æ™°ã€ä¸“ä¸šçš„æ€»ç»“ã€‚"),
                        MessagesPlaceholder(variable_name="messages"),
                        ("system", "å·¥å…·æ‰§è¡Œç»“æœï¼š\n{tool_result}"),
                    ])

                    summary_chain = summary_prompt | self.llm | StrOutputParser()

                    summary_input = {
                        "messages": messages,
                        "tool_result": json.dumps(tool_result, ensure_ascii=False, indent=2)
                    }

                    final_response = ""
                    async for chunk in summary_chain.astream(summary_input):
                        final_response += chunk
                        yield chunk

                    # å®Œæˆ
                    return
                else:
                    yield f"âŒ æ‰¾ä¸åˆ°å·¥å…·: {tool_name}"
                    return
        except (json.JSONDecodeError, Exception) as e:
            logger.warning(f"æ— æ³•è§£æå·¥å…·è°ƒç”¨ï¼Œä½¿ç”¨ç›´æ¥å›å¤: {e}")

        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å› LLM å“åº”
        direct_prompt = ChatPromptTemplate.from_messages([
            ("system", self._build_system_prompt()),
            MessagesPlaceholder(variable_name="messages"),
        ])

        direct_chain = direct_prompt | self.llm | StrOutputParser()

        response = ""
        async for chunk in direct_chain.astream({"messages": messages}):
            response += chunk
            yield chunk

        # å®Œæˆ
        return


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import asyncio

    async def test():
        agent = SecOpsLangChainAgent(
            api_key="your-api-key",
            model="qwen-plus"
        )

        async for chunk in agent.astream_chat(
            message="å¯¹ example.com è¿›è¡Œå®‰å…¨è¯„ä¼°",
            user_id="test_user"
        ):
            print(chunk, end="")

    asyncio.run(test())